            **图形处理器(GPU)的历史、现状和展望**
            以Nvidia为例

引言
===
GPU（Graphics Process Unit，图形处理单元）是一种专门的电子元件，最初用于视频卡上或嵌入在主板、手机、个人计算机和工作站上
以加速计算机图形和图像处理工作。在图像处理领域，最初，GPU主要用于图形渲染和加速计算机游戏的图形效果。它们专注于处理三角形网格、纹理映射、光照等图形相关任务。
随着技术的进步，GPU引入了可编程着色器，如顶点着色器和像素着色器。这允许开发人员对图形渲染的各个阶段进行自定义处理，从而实现更高级的图形效果。
GPU的发展推动了更多高级渲染技术的出现，例如几何着色器、细分着色器、计算着色器等。这些技术提供了更多的灵活性和效果，使得图形渲染更加逼真和真实。

随着人们对算力的不断提高，GPU的并行结构也逐渐被人广泛用于更多任务中去。人们开始探索将GPU用于通用计算任务，称为通用GPU（GPGPU）。
GPU的并行处理能力和高带宽内存使其在许多科学计算、数据处理和机器学习等领域表现出色。而如CUDA、OpenCL等通用GPU编程框架的出现，使并行计算
的开发变得更简单。最近几年，GPU在人工智能和深度学习方面取得了巨大成功。深度神经网络的训练和推断任务可以高度并行化，GPU的强大计算能力使其成为进行大规模深度学习的首选工具。

NVIDIA 是一家全球领先的图形处理器和人工智能计算技术公司，其在显卡领域的发展经历了多个阶段和重要里程碑，从而在图形性能、光线追踪、并行计算和深度学习领域都
有着最前沿的技术和产品，拓展了应用领域。是二十一世纪最成功的公司之一。

本文将以英伟达的显卡产品发展历程为例，总结图形处理器的发展历程，概述图形处理器的主要功能原理和应用，展望未来可能的发展。

图形处理中的GPU
====

在显卡诞生之前，人们在电脑上对三维世界的探索需要巧妙地操纵2D图形，通过缩放贴图对象、一行一行地扭曲像素来创造一种有深度的幻觉。
当处理器的计算能力变得越来越强大，多边形网格才真正被引入，这对动画/游戏行业来说有着革命性的影响。
在GPU诞生之前，这些多边形都是在CPU上处理的。20世纪末出现了许多3D加速卡，可以代替CPU，将传入的顶点数据和纹理光栅化为2D像素显示在屏幕上。
但在此之前CPU仍然需要创建所有对象的可见列表，并将3维空间的物体变换到2位坐标。这个时候的CPU尚未进入多核时代，在创建场景之外，CPU仍然需要在单独的线程
中执行游戏逻辑和底层操作系统相关的其它后台任务。

从诞生到可编程
----
在1999年8月31日，英伟达推出了GeForce 256，并首次提出了“GPU”的概念：“集成了变换，光照，三角形设置/裁剪和渲染引擎的单芯片处理器，能够每秒至少处理1000万个多边形”，也首次将
图形处理器提升到和硬盘、内存、CPU类似的地位。GeForce 256可以同时处理4个像素的光栅化和纹理着色，支持顶点变换和正方形环境贴图。
虽然GeForce 256的主频只有120 MHz，支持的贴图数量只有4，甚至一开始使用的是频率为166 MHz 32MB的SDR显存 （在后来又推出了32MB DDR 版本的GeForce 256，读写速度的提高大幅提升了性能），
但它仍然是当时最快的图形处理器，对多边形的处理速度超过了当时最顶尖的CPU。

固定的渲染管线极大地限制了GPU的作用，所以提高可编程性成为了图形API和GPU发展的主要方向。
在2002年，NVIDIA推出的GeForce 3加入了顶点着色器和可配置的片元管线，也拥有了经典的多重采样抗锯齿（Multisample Anti-aliasing）五点型抗锯齿（Quincunx Anti-aliasing）和功能。
2003年推出的GeForce FX系列支持了32位可编程的片元着色器，也是NVIDIA第一代支持DX 9的硬件设备。

DirectX 9是21世纪初最广泛使用、最有影响力的图形API之一，后续NVIDIA发布的显卡都需要适用于DX 9的设计。直到GeForce 7800 GTX的发布之前，
GeForce的主要升级都在于制程的提升、显存的大小和带宽提升、核心的频率和并行管线的数量增加。

传统GPU架构
----

GeForce 6800是这个时代NVIDIA最成功的GPU之一，我们可以从它的设计来总结传统的专注于图形处理的GPU的最终形态。

对于从命令流中获得的每个顶点，GPU会开辟一个线程运行一个独立的顶点着色器（对每个光栅化后的片段同理）。每个线程有专有的只写的输出寄存器，将结果传送给下一阶段。
除了这些输入输出寄存器，每个线程也有私有的临时寄存器、只读的程序变量和访问过滤后的、重采样的纹理贴图的权限。

![Figure [6800]: GeForce 6800 block diagram.](images/6800_arch.png)

传统的GPU架构含有六个独立的可编程顶点处理器，读入的数据与指令可以分发给任一闲置的顶点处理器，所以并行性几乎是完美的。顶点阶段的处理结果按照应用既定的顺序被重新组装，送到三角形装配和光栅化单元。
对于每个图元，光栅器找到组成的像素片段并将它们送到片段处理器。因为着色阶段所有处理器是彼此独立的，16个可编程的片段处理器可以完美地并行处理所有的任务。最后，交叉条将来自片段处理器的颜色和深度结果分发到十六个
固定功能的像素混合单元，这些单元执行帧缓冲操作，如颜色混合、抗锯齿和模板测试和更新。任一片段着色器的处理结果可以被送到任何位置的帧缓冲中。

### 顶点处理器

![Figure [Vertex]: GeForce 6800 Vertex Processor](images/vertex_arch.png)
顶点处理器可以执行非常大的指令字（为什么是123bit，很奇怪）。如上图所示，每个处理器的数据路径由一个向量乘加单元、一个标量特殊函数单元和一个纹理单元组成。
向量单元可以同时执行四个数学运算。特殊函数单元实现如正弦、余弦对数等超越函数。计算单元可以从512*128 bit的常量RAM、最大达32 * 128bit的临时寄存器，或是16 * 128bit的输入寄存器中获取操作数，
将结果写入临时寄存器或是16*128 bit的输出寄存器中。此外顶点处理器还支持实例化操作。

顶点处理器维护了一个与实现无关的编程模型，使用线程来使数据路径看起来具有统一的延迟，并使用计分板技术（scoreboarding）来隐藏纹理获取的延迟。
顶点处理器的实现是完全多指令多数据（multiple instruction, multiple data，简称MIMD）的。

> **为什么需要一个统一的延迟**？
> 因为可以为编程人员提供一个简单的抽象层，方便将计算任务分解为相同延迟进行的线程，使编程模型更易于理解与实现；
> 可以隐藏不同指令和操作之间的真实延迟，可以简化程序的设计和分析；且线程间可以独立执行，不受其它线程的延迟影响，提高计算性能和吞吐量。

### 片元装配和光栅化

这一阶段的行为由API唯一确定，不要求可编程性，所以可以通过固定功能的单元高效地完成。输入是我们熟悉的齐次坐标向量。同时在这一阶段，装配好的图元
需要进行一次视锥剔除以减少绘制数量。之后再进行视口变换和光栅化，得到一系列片段，输入片段处理器的数组。

> **页友好的光栅化顺序。**
> 第一次见到这个神奇的顺序还是在光线追踪实现的黑魔法上，通过这样的方式遍历像素可以获得很大的提升。没想到原来是这里先用上了。

值得注意的是，NVIDIA自GeForce 3系开始就在这一阶段引入了一个Z-Cull单元，以快速去除粗粒度上被遮挡的片元。而片段处理器阶段指向Z-Cull单元的目的正是更新
其中缓存的部分深度值。

### 片段处理器

片段处理器接收输入的数据（位置、颜色、深度和10种通用的4*FP32的属性），并对三角形的各项属性进行插值和纹理采样。
GeForce 6800的片段处理器可以将结果输出给至多4个目标缓冲。和顶点处理器很类似，片段处理器也拥有常量/临时寄存器资源，和类似的MIMD功能。

![Figure [Fragment]: GeForce 6800 Fragment Processor](images/fragment_arch.png)

由于颜色成分中对Alpha值的不同处理（Alpha Component），fig. [Fragment]中计算单元被分成了两个部分，在“Vector and special-function unit”中支持各种不同的向量运算。
每个片段处理器还支持流水线指令级并行，每个时钟周期内每像素可以执行最多6条DX 9指令。

纹理单元是片段着色器的重要组成部分，为了减小内存阻塞，纹理单元实现了管线式的二级缓存，并对纹理格式进行固定比率的有损压缩以提高细粒度的访问和随机可寻址性。
GeForce 6800支持最近采样、双线性采样、三线性采样和各向异性滤波。

### 像素引擎（Pixel Engine）和光栅操作（Raster Operation）

GeForce 6800拥有16个像素引擎，这些固定功能的单元执行深度/模板检测和更新，以及颜色混合。

每个像素引擎连接到一个特定的内存分区，当抗锯齿启用时，像素引擎会将每个片段的颜色和深度信息扩展至多个样本。GeFORCE 6800支持MSAA和SSAA。SSAA会为每个子样本重新执行片段着色器以得到准确颜色，
而MSAA需要对额外的遮盖性样本进行类似的深度测试和写入。
![Figure [ROP]: GeForce 6800 Pixel Engine](images/pixel_arch.png)

> **为什么要把深度测试放到最后。** 看到过的很有意思的讨论。从前面的架构介绍可以看到，顶点着色器和片段着色器几乎是百分百并行。而深度测试是一个原子性操作，涉及到原子丢弃，原子交换（比较后）。这就涉及到两个电路。
> 而且输入像素着色器的片段需要保持一定的顺序，所以当一个片元正在进行原子交换时，后续的片元就必须等待。造成较低的吞吐量。而现有的z-cull单元是基于粗粒度的保守估计的深度测试，只能剔除部分像素，可以保持比较高的吞吐量。
> 而针对深度的优化还可以先生成只含深度的贴图信息（称为z pre-pass），给第二个pass节省时间，这里就涉及到片段着色器与纹理单元单宽的取舍问题。


Tesla架构
-----

在GPU发展的初期，顶点处理器和像素片段处理器以不同的速度发展：顶点处理器是为低延迟、高精度的数学运算而设计的，而像素片段处理器是为高延迟、低精度的纹理过滤而优化的。
顶点处理器传统上支持更复杂的处理，所以它们比片段处理器更早支持可编程。在发展的过程中，由于需要更强大的编程通用性，这两种处理器类型在功能上已经趋同。另一方面，因为GPU通常处理比顶点更多的像素，所以片段处理器的数量通常是顶点处理器的三倍，然而工作负载不能很好地平衡，
这导致更低的效率。例如对于小三角形，顶点着色器工作量大，片段着色器工作量小，反之同理。且Dx 10中增加了更复杂的几何单元处理（例如引入了几何着色器（Geometry Shader））,使得选择固定的处理器比例变得更困难。
这些都影响设计统一架构的决定。

![Figure [Tesla]: Tesla Architecture](images/tesla_arch.png)

Tesla架构统一且扩展了顶点和像素处理器，构成了可扩展的处理器列。在这里分为流处理器和其它部分介绍。

### 流水线组件

- **主机接口（Host Interface）**负责收发来自CPU的指令（Command），从显存中获取数据，检查指令连贯性，并处理GPU的上下文切换。
- **输入装配器（Input Assembler）**组装顶点数据和顶点属性，传给顶点任务分配器。
- **顶点/像素/计算任务分配器（Vertex、Pixel、Compute Work Distribution）**负责将顶点/片元/计算着色器的任务分发给流处理器完成。
- **纹理处理簇（Texture Processing Clusters，TPC）**包含了一个纹理单元和两个流处理器，在下一小结会重点介绍。
- **视口/裁剪/装配/光栅化/深度剔除单元（Viewport/clip/setup/raster/zcull block）**组成了传统的光栅化单元，将顶点或几何着色器的结果进行转换和剔除。几何原语被分为一个较小的像素块，既方便快速判断剔除，也通过块地址快速选择处理的流处理器。处理完的像素会被打包通过流处理器控制器分配TPC进行片段着色器的工作。
- **光栅运算处理器（Raster Operations Processor）**功能与传统管线非常类似。每个ROP被分配了一个特定的内存区块，TPC通过互联网络向ROP传送着色像素。
- **L2 Cache、Memory Controller和DRAM**组成的存储系统。每个DRAM搭配了一个Memory Controller、L2 Cache和一个ROP，处理从TPC的SM里发来的数据吞吐请求和ROP写入帧缓冲的像素数据。

### 流处理器

流处理器由以下几个单元组成：
- **几何控制器（Geometry Controller）**负责顶点属性在芯片内的输入输出，它会把顶点/几何着色器的结果送到光栅化单元，或者保存到内存中。
- **流处理器控制器（SM Controller）**负责将各种计算任务拆分打包成Warp，交给TPC中所属的某个SM来计算。此外，SMC还负责协调

通用计算中的GPU
====

随着GPU性能的不断提高，将GPU用于更广泛的通用计算成为了一个主要趋势。相比CPU，GPU的浮点计算速度比CPU快得多，且能有更好的负载平衡——将计算任务分配给GPU可独立于CPU完成计算，而且GPU性能的增长曲线远远快于CPU。

不同于CPU的线性处理模式，GPU采用的模型通常是流式模型（Stream Processor），在一组输入数据上并行地执行一个特定的函数，输出一组结果。在这个过程中，函数通常被称为核（kernel），一组数据通常被称为流（stream）。
数据以流的形式输入处理器，在核函数上被执行，结果输出到内存中。所有传入处理器的元素独立地被处理，这保证了GPU架构不用向使用者暴露任何并行单元和组成，就可以实现一个完全并行的编程模型。

可编程管线时代的通用计算
----

在可编程管线诞生时，已经有相当一部分任务可以将数据抽象为“片段”，在片段着色器中并行计算。如Simon Green在2003年展示了用这样的技术在GPU上模拟布料。模拟算法的核心是基于GPU的双调排序和二分查找，这里就不再展开。
而Purcell等人在这时已经开始了光线追踪的尝试。他们将光线追踪的过程分为四个核：光线生成、均匀网格遍历、三角形求交和着色。
- 光线生成。给定相机参数，对于每个屏幕像素生成一条出射光线，并检测是否与场景包围盒相交。
- 均匀网格遍历。选择均匀网格作为加速结构的理由是，各种加速结构的效率没有绝对优势，且均匀网格在GPU的实现上更为简单，在遍历时也有3维的DDA算法。![Figure [grid]The grid and triangle data structures stored in texture
memory.](images/ray_trace.png)
- 三角形求交。对于含有三角形的网格，对体素中包含的所有三角形进行相交测试。
- 着色阶段。场景的材质和额外的顶点信息存在一系列的RGB纹理中。对于Path Tracer，散射的光线会被存在内存中以在场景中继续传播。

这样的光线追踪管线的表现相当不错，且还能在今天的一些基于Computer Shader的光追样例中看到架构/存储上的相似之处。





展望
===


(#) 引用

[#GeForce 6800]: J. Montrym and H. Moreton, "The GeForce 6800," in IEEE Micro, vol. 25, no. 2, pp. 41-51, March-April 2005, doi:
10.1109/MM.2005.37.
[#GPU Gems]: Randima Fernando. 2004. GPU Gems: Programming Techniques, Tips and Tricks for Real-Time Graphics. Pearson Higher
Education.
[#GPU Gems2]: Matt Pharr and Randima Fernando. 2005. GPU Gems 2: Programming Techniques for High-Performance Graphics and
General-Purpose Computation (Gpu Gems). Addison-Wesley Professional.
[#GPU Gems3]: Hubert Nguyen. 2007. Gpu gems 3 (First. ed.). Addison-Wesley Professional.
[#Simon]: Green, S. (2003, March). Stupid opengl shader tricks. In Game Development Conference (Vol. 2003).
[#Purcell]: Purcell, Timothy J., Ian Buck, William R. Mark, and Pat Hanrahan. 2002. "Ray Tracing on Programmable Graphics Hardware." ACM Transactions on Graphics 21(3), pp. 703–712.

<link rel="stylesheet" href="https://morgan3d.github.io/markdeep/latest/latex.css?">
<!-- Markdeep: --><style class="fallback">body{visibility:hidden}</style><script src="https://morgan3d.github.io/markdeep/latest/markdeep.min.js?" charset="utf-8"></script>