            **图形处理器(GPU)的历史、现状和展望**
            以Nvidia为例

引言
===
GPU（Graphics Process Unit，图形处理单元）是一种专门的电子元件，最初用于视频卡上或嵌入在主板、手机、个人计算机和工作站上
以加速计算机图形和图像处理工作。在图像处理领域，最初，GPU主要用于图形渲染和加速计算机游戏的图形效果。它们专注于处理三角形网格、纹理映射、光照等图形相关任务。
随着技术的进步，GPU引入了可编程着色器，如顶点着色器和像素着色器。这允许开发人员对图形渲染的各个阶段进行自定义处理，从而实现更高级的图形效果。
GPU的发展推动了更多高级渲染技术的出现，例如几何着色器、细分着色器、计算着色器等。这些技术提供了更多的灵活性和效果，使得图形渲染更加逼真和真实。

随着人们对算力的不断提高，GPU的并行结构也逐渐被人广泛用于更多任务中去。人们开始探索将GPU用于通用计算任务，称为通用GPU（GPGPU）。
GPU的并行处理能力和高带宽内存使其在许多科学计算、数据处理和机器学习等领域表现出色。而如CUDA、OpenCL等通用GPU编程框架的出现，使并行计算
的开发变得更简单。最近几年，GPU在人工智能和深度学习方面取得了巨大成功。深度神经网络的训练和推断任务可以高度并行化，GPU的强大计算能力使其成为进行大规模深度学习的首选工具。

NVIDIA 是一家全球领先的图形处理器和人工智能计算技术公司，其在显卡领域的发展经历了多个阶段和重要里程碑，从而在图形性能、光线追踪、并行计算和深度学习领域都
有着最前沿的技术和产品，拓展了应用领域。是二十一世纪最成功的公司之一。

本文将以英伟达的显卡产品发展历程为例，总结图形处理器的发展历程，概述图形处理器的主要功能原理和应用，展望未来可能的发展。

图形处理中的GPU
====

在显卡诞生之前，人们在电脑上对三维世界的探索需要巧妙地操纵2D图形，通过缩放贴图对象、一行一行地扭曲像素来创造一种有深度的幻觉。
当处理器的计算能力变得越来越强大，多边形网格才真正被引入，这对动画/游戏行业来说有着革命性的影响。
在GPU诞生之前，这些多边形都是在CPU上处理的。20世纪末出现了许多3D加速卡，可以代替CPU，将传入的顶点数据和纹理光栅化为2D像素显示在屏幕上。
但在此之前CPU仍然需要创建所有对象的可见列表，并将3维空间的物体变换到2位坐标。这个时候的CPU尚未进入多核时代，在创建场景之外，CPU仍然需要在单独的线程
中执行游戏逻辑和底层操作系统相关的其它后台任务。

从诞生到可编程
----
在1999年8月31日，英伟达推出了GeForce 256，并首次提出了“GPU”的概念：“集成了变换，光照，三角形设置/裁剪和渲染引擎的单芯片处理器，能够每秒至少处理1000万个多边形”，也首次将
图形处理器提升到和硬盘、内存、CPU类似的地位。GeForce 256可以同时处理4个像素的光栅化和纹理着色，支持顶点变换和正方形环境贴图。
虽然GeForce 256的主频只有120 MHz，支持的贴图数量只有4，甚至一开始使用的是频率为166 MHz 32MB的SDR显存 （在后来又推出了32MB DDR 版本的GeForce 256，读写速度的提高大幅提升了性能），
但它仍然是当时最快的图形处理器，对多边形的处理速度超过了当时最顶尖的CPU。

固定的渲染管线极大地限制了GPU的作用，所以提高可编程性成为了图形API和GPU发展的主要方向。
在2002年，NVIDIA推出的GeForce 3加入了顶点着色器和可配置的片元管线，也拥有了经典的多重采样抗锯齿（Multisample Anti-aliasing）五点型抗锯齿（Quincunx Anti-aliasing）和功能。
2003年推出的GeForce FX系列支持了32位可编程的片元着色器，也是NVIDIA第一代支持DX 9的硬件设备。

DirectX 9是21世纪初最广泛使用、最有影响力的图形API之一，后续NVIDIA发布的显卡都需要适用于DX 9的设计。直到GeForce 7800 GTX的发布之前，
GeForce的主要升级都在于制程的提升、显存的大小和带宽提升、核心的频率和并行管线的数量增加。

传统GPU架构
----

GeForce 6800是这个时代NVIDIA最成功的GPU之一，我们可以从它的设计来总结传统的专注于图形处理的GPU的最终形态。

对于从命令流中获得的每个顶点，GPU会开辟一个线程运行一个独立的顶点着色器（对每个光栅化后的片段同理）。每个线程有专有的只写的输出寄存器，将结果传送给下一阶段。
除了这些输入输出寄存器，每个线程也有私有的临时寄存器、只读的程序变量和访问过滤后的、重采样的纹理贴图的权限。

![Figure [6800]: GeForce 6800 block diagram.](images/6800_arch.png)

传统的GPU架构含有六个独立的可编程顶点处理器，读入的数据与指令可以分发给任一闲置的顶点处理器，所以并行性几乎是完美的。顶点阶段的处理结果按照应用既定的顺序被重新组装，送到三角形装配和光栅化单元。
对于每个图元，光栅器找到组成的像素片段并将它们送到片段处理器。因为着色阶段所有处理器是彼此独立的，16个可编程的片段处理器可以完美地并行处理所有的任务。最后，交叉条将来自片段处理器的颜色和深度结果分发到十六个
固定功能的像素混合单元，这些单元执行帧缓冲操作，如颜色混合、抗锯齿和模板测试和更新。任一片段着色器的处理结果可以被送到任何位置的帧缓冲中。

### 顶点处理器

![Figure [Vertex]: GeForce 6800 Vertex Processor](images/vertex_arch.png)
顶点处理器可以执行非常大的指令字（为什么是123bit，很奇怪）。如上图所示，每个处理器的数据路径由一个向量乘加单元、一个标量特殊函数单元和一个纹理单元组成。
向量单元可以同时执行四个数学运算。特殊函数单元实现如正弦、余弦对数等超越函数。计算单元可以从512*128 bit的常量RAM、最大达32 * 128bit的临时寄存器，或是16 * 128bit的输入寄存器中获取操作数，
将结果写入临时寄存器或是16*128 bit的输出寄存器中。此外顶点处理器还支持实例化操作。

顶点处理器维护了一个与实现无关的编程模型，使用线程来使数据路径看起来具有统一的延迟，并使用计分板技术（scoreboarding）来隐藏纹理获取的延迟。
顶点处理器的实现是完全多指令多数据（multiple instruction, multiple data，简称MIMD）的。

> **为什么需要一个统一的延迟**？
> 因为可以为编程人员提供一个简单的抽象层，方便将计算任务分解为相同延迟进行的线程，使编程模型更易于理解与实现；
> 可以隐藏不同指令和操作之间的真实延迟，可以简化程序的设计和分析；且线程间可以独立执行，不受其它线程的延迟影响，提高计算性能和吞吐量。

### 片元装配和光栅化

这一阶段的行为由API唯一确定，不要求可编程性，所以可以通过固定功能的单元高效地完成。输入是我们熟悉的齐次坐标向量。同时在这一阶段，装配好的图元
需要进行一次视锥剔除以减少绘制数量。之后再进行视口变换和光栅化，得到一系列片段，输入片段处理器的数组。

> **页友好的光栅化顺序。**
> 第一次见到这个神奇的顺序还是在光线追踪实现的黑魔法上，通过这样的方式遍历像素可以获得很大的提升。没想到原来是这里先用上了。

值得注意的是，NVIDIA自GeForce 3系开始就在这一阶段引入了一个Z-Cull单元，以快速去除粗粒度上被遮挡的片元。而片段处理器阶段指向Z-Cull单元的目的正是更新
其中缓存的部分深度值。

### 片段处理器

片段处理器接收输入的数据（位置、颜色、深度和10种通用的4*FP32的属性），并对三角形的各项属性进行插值和纹理采样。
GeForce 6800的片段处理器可以将结果输出给至多4个目标缓冲。和顶点处理器很类似，片段处理器也拥有常量/临时寄存器资源，和类似的MIMD功能。

![Figure [Fragment]: GeForce 6800 Fragment Processor](images/fragment_arch.png)

由于颜色成分中对Alpha值的不同处理（Alpha Component），fig. [Fragment]中计算单元被分成了两个部分，在“Vector and special-function unit”中支持各种不同的向量运算。
每个片段处理器还支持流水线指令级并行，每个时钟周期内每像素可以执行最多6条DX 9指令。

纹理单元是片段着色器的重要组成部分，为了减小内存阻塞，纹理单元实现了管线式的二级缓存，并对纹理格式进行固定比率的有损压缩以提高细粒度的访问和随机可寻址性。
GeForce 6800支持最近采样、双线性采样、三线性采样和各向异性滤波。

### 像素引擎（Pixel Engine）和光栅操作（Raster Operation）

GeForce 6800拥有16个像素引擎，这些固定功能的单元执行深度/模板检测和更新，以及颜色混合。

每个像素引擎连接到一个特定的内存分区，当抗锯齿启用时，像素引擎会将每个片段的颜色和深度信息扩展至多个样本。GeFORCE 6800支持MSAA和SSAA。SSAA会为每个子样本重新执行片段着色器以得到准确颜色，
而MSAA需要对额外的遮盖性样本进行类似的深度测试和写入。
![Figure [ROP]: GeForce 6800 Pixel Engine](images/pixel_arch.png)

> **为什么要把深度测试放到最后。** 看到过的很有意思的讨论。从前面的架构介绍可以看到，顶点着色器和片段着色器几乎是百分百并行。而深度测试是一个原子性操作，涉及到原子丢弃，原子交换（比较后）。这就涉及到两个电路。
> 而且输入像素着色器的片段需要保持一定的顺序，所以当一个片元正在进行原子交换时，后续的片元就必须等待。造成较低的吞吐量。而现有的z-cull单元是基于粗粒度的保守估计的深度测试，只能剔除部分像素，可以保持比较高的吞吐量。
> 而针对深度的优化还可以先生成只含深度的贴图信息（称为z pre-pass），给第二个pass节省时间，这里就涉及到片段着色器与纹理单元单宽的取舍问题。


Tesla架构
-----

在GPU发展的初期，顶点处理器和像素片段处理器以不同的速度发展：顶点处理器是为低延迟、高精度的数学运算而设计的，而像素片段处理器是为高延迟、低精度的纹理过滤而优化的。
顶点处理器传统上支持更复杂的处理，所以它们比片段处理器更早支持可编程。在发展的过程中，由于需要更强大的编程通用性，这两种处理器类型在功能上已经趋同。另一方面，因为GPU通常处理比顶点更多的像素，所以片段处理器的数量通常是顶点处理器的三倍，然而工作负载不能很好地平衡，
这导致更低的效率。例如对于小三角形，顶点着色器工作量大，片段着色器工作量小，反之同理。且Dx 10中增加了更复杂的几何单元处理（例如引入了几何着色器（Geometry Shader））,使得选择固定的处理器比例变得更困难。
这些都影响设计统一架构的决定。

Tesla架构统一并且扩展了顶点和像素处理器，


通用计算中的GPU
====




展望
===


(#) 引用

[#GeForce 6800]: J. Montrym and H. Moreton, "The GeForce 6800," in IEEE Micro, vol. 25, no. 2, pp. 41-51, March-April 2005, doi:
10.1109/MM.2005.37.
[#GPU Gems]: Randima Fernando. 2004. GPU Gems: Programming Techniques, Tips and Tricks for Real-Time Graphics. Pearson Higher
Education.
[#GPU Gems2]: Matt Pharr and Randima Fernando. 2005. GPU Gems 2: Programming Techniques for High-Performance Graphics and
General-Purpose Computation (Gpu Gems). Addison-Wesley Professional.
[#GPU Gems3]: Hubert Nguyen. 2007. Gpu gems 3 (First. ed.). Addison-Wesley Professional.



<link rel="stylesheet" href="https://morgan3d.github.io/markdeep/latest/latex.css?">
<!-- Markdeep: --><style class="fallback">body{visibility:hidden}</style><script src="https://morgan3d.github.io/markdeep/latest/markdeep.min.js?" charset="utf-8"></script>